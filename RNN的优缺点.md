10.9

## RNN的优缺点

### 优点

![image-20241009101618970](C:\Users\liyuz\AppData\Roaming\Typora\typora-user-images\image-20241009101618970.png)

1.因为需要接受上一个时间步的状态，上一个状态又是接受上上个时间步的状态。这就是记忆力的体现

2.因为有记忆力，所以才能接受这种需要记忆力的时序数据

3.这是记忆力的基础

4.单个神经元的共享权重，也就是说无论是那个时间步，只要是在这个神经元中，权重矩阵就只有这个，不会改变。

5.所有时间步都走完了，循环结束

### 缺点

<img src="C:\Users\liyuz\AppData\Roaming\Typora\typora-user-images\image-20241009103350689.png" alt="image-20241009103350689" style="zoom:50%;" />

rnn的时间步：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/8fda62dcbd6aa6670f38a440756d774f.png)

也就是每一个时间步的状态都会占据一些份额。

但是并不是这样占据相同的份额，而是随着时间推移，最前面的占比越少，最近的时间步（也就是上一个时间步）对当前状态的影响最大，**这就是短期记忆力问题**

RNN的神经元提取的特征最主要的是时间步之间的依赖

而提取的特征使用权重矩阵来表示的，反向传播，梯度下降也是为了调整权重矩阵。

==下面的知识是介绍梯度消失的，因为连乘所以导致梯度消失，所以越靠前的时间步梯度越容易消失，所以说网络就学习不到跟最前面的时间步的依赖关系，也就是说网络不认为自己跟前面有关，也就是失忆了，只能记住短期的，因为短期梯度没有消失，这就是为什么短期记忆==

#### **补充一些rnn的反向传播知识：**

在梯度下降算法中，确实是通过减去 梯度和学习率的乘积 来更新网络参数的。但是，在讨论梯度消失和梯度爆炸问题时，我们**关注的是梯度的传播过程**，特别是在反向传播中梯度的变化。

当我们说“梯度连乘”时，实际上是指梯度在反向传播过程中如何通过每一层传递，并受到每一层激活函数导数的影响。这里的关键点是，梯度在每一层都是通过乘法来组合的，而不是在最终的参数更新步骤中。

![image-20241009110927269](C:\Users\liyuz\AppData\Roaming\Typora\typora-user-images\image-20241009110927269.png)

1.首先我们计算梯度是激活函数关于状态值的偏导数

所以受限于不同的激活函数，梯度也是有上限的

根据第三步提供的公式，我们可以看出来最初的时间步就是来自后面的连乘。

## 补充激活函数和损失函数的关系!

在深度学习中，损失函数和激活函数在梯度计算中的角色和应用方式是不同的，但它们之间存在联系。以下是它们在计算梯度时的应用和关系：
### 损失函数的应用
1. **定义目标**：损失函数定义了模型预测值与真实值之间的差异，是优化的目标函数。
2. **计算损失**：在训练过程中，对于每个输入样本，模型会输出一个预测值，然后计算预测值与真实值之间的损失。
3. **梯度计算**：为了最小化损失，我们需要计算损失函数关于模型参数的梯度。这是通过反向传播算法实现的，它从输出层开始，计算损失函数相对于每个参数的梯度。
### 激活函数的应用
1. **引入非线性**：激活函数在神经网络的每个神经元后引入非线性，使得模型能够学习复杂的函数。
2. **计算激活梯度**：在反向传播过程中，除了计算损失函数关于模型参数的梯度外，还需要计算激活函数的梯度。这是因为激活函数是模型参数的非线性变换。
### 两者之间的联系
1. **链式法则**：在反向传播过程中，损失函数的梯度是通过链式法则与激活函数的梯度相乘来计算的。具体来说，损失函数相对于某个参数的梯度是由损失函数相对于该参数影响的输出（即激活函数的输出）的梯度，乘以激活函数相对于该参数的梯度得到的。
   如果用数学表达式来表示，对于第 \( l \) 层的参数 \( \theta_l \)，梯度计算可以表示为：
   $\frac{\partial L}{\partial \theta_l} = \frac{\partial L}{\partial a_l} \cdot \frac{\partial a_l}{\partial z_l} \cdot \frac{\partial z_l}{\partial \theta_l}$
   ![image-20241009114635756](C:\Users\liyuz\AppData\Roaming\Typora\typora-user-images\image-20241009114635756.png)
2. **梯度消失/爆炸**：激活函数的选择对梯度在反向传播中的传递有重要影响。如果激活函数的导数很小（小于1），可能会导致梯度消失，使得深层的参数难以更新。相反，如果激活函数的导数很大，可能会导致梯度爆炸，使得训练过程不稳定。
### 总结
在梯度计算中，损失函数和激活函数共同作用：
- **损失函数**定义了优化目标，并提供了最终输出层的梯度。

- **激活函数**的梯度用于在反向传播过程中将损失函数的梯度传递到网络中的每一层。
  通过链式法则，这两个函数的梯度被组合起来，以计算网络中所有参数的梯度，从而指导参数的更新。

- 
  在深度学习中，损失函数和激活函数的应用确实遵循以下模式：

  1. **激活函数应用于每一层的输出**：
     - 在一个神经网络中，除了输入层之外，每个隐藏层和输出层在经过线性变换（例如权重和偏置的线性组合）之后，都会应用一个激活函数。**激活函数引入了非线性，使得神经网络能够学习和表示复杂的函数。**
     - 对于每个隐藏层，激活函数的输出将作为下一层的输入。
  2. **损失函数应用于最后**：
     - 损失函数是用来衡量模型预测值与真实值之间差异的函数。在网络的最后一层（通常是输出层）之后应用损失函数。
     - 在训练过程中，网络的最终输出会与真实标签值进行比较，通过损失函数计算出损失值，这个损失值反映了模型在当前参数下的性能。
     因此，可以这样理解：
  - **正向传播**：输入数据通过网络的每一层，每一层都进行线性变换后应用激活函数，直到最后一层输出预测值。
  - **损失计算**：将最后一层的预测值与真实值比较，通过损失函数计算损失。
  - **反向传播**：从损失函数开始，通过链式法则计算每一层的梯度，这个过程中需要使用每一层的激活函数的梯度。
  这种结构确保了神经网络能够通过梯度下降等优化算法进行有效的训练和参数更新。

  $h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)$就是一个激活函数，其实就是给正常输出套上了一个非线性的壳子



言归正传，重新看rnn的反向传播

==那么在链式法则中，损失函数关于最初状态的梯度就是激活函数关于之后所有的状态的偏导数相乘==

但是这就会出现一个问题，如果时间序列过长，且激活函数的导数的上限小于1，那么连乘之后，梯度就会逐渐趋近0

那么梯度下降算法就没法应用了，因为减不减的没用

==注意此时的梯度=0并不是意味着网络收敛，因为网络收敛是梯度下降不断地调整的权重，也就是不断地学习最终使得损失函数取得最小值；但是现在是梯度消失，也就是还没学习呢，你的梯度没法下降了，相当于你什么都没学到，就没法动了==

就像是一台抽水机，正常是把水抽干了之后才没法继续工作了

而梯度消失是一上来就不工作了。

虽然最后的结果都是停止工作，但是本质不同。

rnn中经常是用tanh作为激活函数，导数上限是1，但是那是上限，也就是≤1，也容易梯度消失。

当然，如果，梯度是＞1的，也会梯度爆炸。爆炸。。。。直接无法收敛。





# rnn模型

![image-20241009121516283](C:\Users\liyuz\AppData\Roaming\Typora\typora-user-images\image-20241009121516283.png)

第一组的公式我们现在已经很清楚了

就是隐藏状态的更新公式。

后面那组我们没见过，其实就是每个时间步的输出。

**我最开始认为神经元的输出就是状态向量，但是这个O其实是针对目标的输出**

比如说在预测下一个词是什么的时候

x1是“我”，x2是“喜欢”。那么o2代表输入了“我喜欢”之后，网络的预测下一个你可能会输入什么，把几种可能性的概率都输出出来，这就是O2

有种一边编码一边解码的感觉，但其实真正的过程，还是先编码，也就是走完所有的时间序列。最后的状态ht，==**我们称之为上下文向量**==

我们是根据这个向量来进行解码的

### 上下文向量

**上下文向量是编码层的最终结果**

根据上下文向量来解码的过程通常涉及使用解码器RNN逐步生成输出序列。以下是一个简化的例子，说明如何使用上下文向量来解码，假设我们正在构建一个英法翻译的序列到序列（Seq2Seq）模型。
#### 例子：英法翻译
输入序列

假设输入的英文句子是 "I love dogs"。

编码器

编码器处理输入序列并生成一个上下文向量C，这通常是最后一个时间步的隐藏状态或所有隐藏状态的聚合。

解码器

现在，我们使用解码器来生成法语的翻译。解码过程如下：
1. **初始化**：
   - 解码器的初始隐藏状态通常设置为编码器的最后一个隐藏状态，即上下文向量C。
   - 解码器的第一个输入通常是特殊的开始符号，比如 `<s>`。
2. **第一个时间步**：
   - 输入：`<s>`（开始符号）
   - 隐藏状态：初始化为C
   - 解码器RNN计算新的隐藏状态，并生成输出O1。
   - 输出O1通过一个全连接层和softmax函数转换为一个概率分布，**表示法语词汇表中每个法语词的概率**。
   - 选择概率最高的词作为输出，例如 "Je"（"I" 的法语翻译）。
3. **第二个时间步**：
   - 输入：现在输入是上一步的输出 "Je"。
   - 隐藏状态：上一步的隐藏状态。
   - 解码器RNN计算新的隐藏状态，并生成输出O2。
   - 输出O2转换为概率分布，选择概率最高的词，例如 "aime"（"love" 的法语翻译）。
4. **第三个时间步**：
   - 输入：上一步的输出 "aime"。
   - 隐藏状态：上一步的隐藏状态。
   - 解码器RNN计算新的隐藏状态，并生成输出O3。
   - 输出O3转换为概率分布，选择概率最高的词，例如 "les"（"the" 的法语翻译，这里假设为了语法正确性，需要在 "dogs" 前面加上 "les"）。
5. **第四个时间步**：
   - 输入：上一步的输出 "les"。
   - 隐藏状态：上一步的隐藏状态。
   - 解码器RNN计算新的隐藏状态，并生成输出O4。
   - 输出O4转换为概率分布，选择概率最高的词，例如 "chiens"（"dogs" 的法语翻译）。
6. **结束**：
   - 当解码器生成结束符号 `</s>` 或达到序列的最大长度时，解码过程停止。

最终输出

通过这个过程，解码器逐步生成了整个法语句子 "Je aime les chiens"。
这个例子展示了如何使用上下文向量来初始化解码器的隐藏状态，并在每个时间步根据前一个输出和当前的隐藏状态来生成新的输出，直到整个输出序列被生成。这个过程可以更复杂，比如使用注意力机制来动态地关注输入序列的不同部分。

是的，您的理解是正确的。在循环神经网络（RNN）中，`Oi` 可以被视为对隐藏状态 `hi` 进行某种形式的“解码”或“激活”之后的产物。这里的“解码”或“激活”通常指的是通过一个或多个以下步骤将隐藏状态转换为一个实际的输出：



## Oi和hi的关系

1. **激活函数**：隐藏状态 `hi` 经常通过一个非线性激活函数（如 tanh、ReLU 或 sigmoid）进行转换，以产生具有所需特性的输出。
   ```
   Oi = activation_function(hi)
   ```
2. **全连接层**：隐藏状态 `hi` 可能通过一个全连接（线性）层，该层包含可学习的权重和偏置，然后将激活函数应用于其输出。
   ```
   Oi = activation_function(W_hi * hi + b)
   ```
   其中 `W_hi` 是权重矩阵，`b` 是偏置项。
3. **softmax 层**：在分类任务中，`Oi` 通常通过一个 softmax 层来生成一个概率分布，表示当前时间步每个可能输出类别的概率。
   ```
   Oi = softmax(W_hi * hi + b)
   ```
   因此，`Oi` 是 `hi` 经过一系列数学变换后的结果，这些变换使得隐藏状态能够适应特定的任务需求，如生成一个分类标签、一个实数值或一个概率分布。在序列到序列模型中，`Oi` 可能是一个中间步骤的输出，而在最终的输出生成阶段，可能还会涉及额外的处理，如使用解码器网络来生成序列。

## Oi的作用

#### 编码过程：

在编码阶段，RNN的每个时间步的输出 `Oi` 通常**==不是==模型最终关注的主要输出**，因为编码器的主要目的是捕捉整个输入序列的表示。然而，`Oi` 在编码阶段仍然可以有一些作用，具体取决于模型的设计和应用场景。以下是一些可能的作用：
1. **中间特征表示**：`Oi` 可以被视为输入序列在时间步 i 的特征表示，它可以用于==分析或理解序列的局部特性==。

2. **辅助损失函数**：在某些情况下，`Oi` 可以用于定义辅助损失函数，这有助于在训练过程中提供额外的监督信息。例如，在序列标注任务中，`Oi` 可以被用来预测每个时间步的标签。

3. **调试和可视化**：在模型开发和调试过程中，==`Oi` 可以用来检查模型是否正确地学习了输入序列的表示==。通过可视化 `Oi`，研究人员可以==更好地理解模型的行为==。

   

   尽管 `Oi` 在编码阶段可能有这些作用，但在标准的Seq2Seq模型中，我们通常只关注编码器最后一个时间步的隐藏状态 `hi`（或所有时间步隐藏状态的聚合），因为它将被用作解码器的上下文向量。解码器使用这个上下文向量来生成输出序列，而不是直接使用编码器每个时间步的 `Oi`。

也就是说我们可以通过每个时间步的输出来看看==网络学到了什么，学的对不对==

#### 解码过程：

在解码过程中，`Oi`（即解码器RNN在时间步 i 的输出）扮演着关键角色，以下是在解码过程中`Oi`的一些主要作用：

1. **生成输出序列**：在解码器中，`Oi` 通常是通过一个非线性激活函数（如softmax）转换后的输出，它代表了在时间步 i 时每个可能输出词汇的概率分布。从这个分布中，可以采样或选择最可能的词汇作为序列的下一个元素。

2. **作为下一个时间步的输入**：在某些解码器设计中，`Oi` 可以被用作下一个时间步的输入（经过适当的处理，比如嵌入层的转换）。这种自回归的解码方式允许模型在生成序列时考虑自己之前生成的元素。

   #### 输入序列
   假设输入的英文句子是 "I love dogs"。
   #### 编码器
   编码器处理输入序列并生成一个上下文向量C，这通常是最后一个时间步的隐藏状态或所有隐藏状态的聚合。
   #### 解码器
   现在，我们使用解码器来生成法语的翻译。解码过程如下：
   1. **初始化**：
      - 解码器的初始隐藏状态通常设置为编码器的最后一个隐藏状态，即上下文向量C。
      - 解码器的第一个输入通常是特殊的开始符号，比如 `<s>`。
   2. **第一个时间步**：
      - 输入：`<s>`（开始符号）
      - 隐藏状态：初始化为C
      - 解码器RNN计算新的隐藏状态，并生成输出O1。
      - 输出O1通过一个全连接层和softmax函数转换为一个概率分布，表示词汇表中每个法语词的概率。
      - ==选择概率最高的词作为输出，例如 "Je"，即O1=“Je”==（"I" 的法语翻译）。
   3. **第二个时间步**：
      - ==输入：现在输入是上一步的输出 "Je"，即X2=O1=“Je”==。
      - 隐藏状态：上一步的隐藏状态。
      - 解码器RNN计算新的隐藏状态，并生成输出O2。
      - 输出O2转换为概率分布，选择概率最高的词，例如 "aime"（"love" 的法语翻译）。
   4. **结束**：
      - 当解码器生成结束符号 `</s>` 或达到序列的最大长度时，解码过程停止。
   #### 最终输出
   通过这个过程，解码器逐步生成了整个法语句子 "Je aime les chiens"。
   这个例子展示了如何使用上下文向量来初始化解码器的隐藏状态，并在每个时间步根据前一个输出和当前的隐藏状态来生成新的输出，直到整个输出序列被生成。这个过程可以更复杂，比如使用注意力机制来动态地关注输入序列的不同部分。

   

3. **损失函数的计算**：在训练过程中，`Oi` 用于计算损失函数，通常是通过比较`Oi`和实际目标序列中对应时间步的标签来计算的。这有助于通过反向传播调整模型参数。

   ==在训练循环神经网络（RNN）时，通常是在每个时间步都计算损失，然后将所有时间步的损失相加以得到整个序列的总损失==。这个过程是必要的，因为RNN处理的是序列数据，每个时间步的输出都与之前的时间步相关联。

   以下是详细的过程：

   1. **前向传播**：在处理序列的每个时间步时，RNN会根据当前的输入和前一个时间步的隐藏状态来计算当前时间步的输出和新的隐藏状态。
   2. **损失计算**：在每个时间步，都会计算当前输出的损失。这通常是通过比较模型==在该时间步==的预测输出（经过softmax处理后的概率分布）和实际目标输出（通常是one-hot编码的标签）来完成的。使用交叉熵损失函数来计算损失。
   3. **累积损失**：将每个时间步的损失==累加==起来，得到整个序列的总损失。
   4. **反向传播**：一旦计算了整个序列的总损失，就会执行反向传播。在反向传播过程中，计算损失相对于每个时间步的输出的梯度，然后进一步计算关于模型参数（如权重和偏置）的梯度。
   5. **参数更新**：使用计算出的梯度来更新模型的参数。

   这个过程的关键在于，每个时间步的损失都反映了模型在==该时间步==的性能，而整个序列的总损失则反映了模型在处理==整个序列时的整体性能==。







# LSTM

1. **输入门（Input Gate）**：控制有多少新的信息被存储在单元状态中。
2. **遗忘门（Forget Gate）**：控制当前单元状态中有多少信息被保留。
3. **输出门（Output Gate）**：决定有多少信息从单元状态中输出。

![img](https://i-blog.csdnimg.cn/direct/c240a80a4f11466f98d379215cc6e7f2.png)

遗忘门：我要根据上个时间步的输出门结果ht-1，加上当前时间步的输出Xt（不是相加，**而是矩阵合并**），来看看那些信息可以遗忘、哪些需要记住

这些东西怎么学习呢，加一个权重矩阵Wf。

随后跟上个时间步的单元状态相乘，来决定怎么遗忘，因为==现在比上个时间步多了Xt，所以要重新计算遗忘==

Ct-1和ht-1是不同的，前者是上个时间步的真正状态（**也就是单元状态**），但是后者是选择性遗忘之后的状态。

输入门：是根据ht-1和Xt来学习当前时间步的遗忘和特征

怎么学习呢？还是用权重矩阵，但是用不同的激活函数，sigmod可以压缩到0-1，适合遗忘（0就是忘记，1就是完全记住）

tanh函数就是用来特征提取。

然后两者相乘，那就是看看当前这个**隐藏状态（别忘了隐藏状态的计算就是来自xi和ht-1，只不过现在的ht-1是遗忘之后的）**怎么遗忘

然后两者相加，更新单元状态。

==感觉就是一个“集成学习”的感觉，根据遗忘的输出建立遗忘权重矩阵来处理上个单元状态，然后同样根据遗忘的输出，来处理输入学习新的特征，然后将两者综合起来，得到最终的单元状态，输入到下个单元==

遗忘门：仍然是根据遗忘的输出将真正的单元状态进行遗忘，输出ht，作为下一个单元的输入

我们三次都是根据遗忘的输出，但是权重矩阵不同，达到的功能不同，分别是用于处理

上个单元状态

当前输入

当前单元状态